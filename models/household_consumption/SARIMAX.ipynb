{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095fe111",
   "metadata": {},
   "source": [
    "# SARIMAX - Análise de Consumo Doméstico de Energia\n",
    "\n",
    "## Objetivo\n",
    "Modelar e prever o consumo doméstico de energia usando SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors)\n",
    "\n",
    "## Dataset\n",
    "- **Arquivo**: household_consumption.xlsx\n",
    "- **Variável Target**: Consumption (consumo de energia)\n",
    "- **Frequência**: Dados por minuto (2006-12-16)\n",
    "- **Tipo**: Série temporal univariada com possível sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a25d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Análise de séries temporais\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Transformações\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "# Métricas de avaliação\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Criar diretório de saída\n",
    "output_dir = '../../../out/household_consumption'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fe238",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Exploração Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5f343",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/household_consumption.'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Carregamento dos dados\u001b[39;00m\n\u001b[32m      2\u001b[39m data_path = \u001b[33m'\u001b[39m\u001b[33m../../../data/household_consumption.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSheet1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInformações básicas do dataset:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joncu\\OneDrive\\Documentos\\series_temporais_projeto\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joncu\\OneDrive\\Documentos\\series_temporais_projeto\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joncu\\OneDrive\\Documentos\\series_temporais_projeto\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joncu\\OneDrive\\Documentos\\series_temporais_projeto\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../../data/household_consumption.'"
     ]
    }
   ],
   "source": [
    "# Carregamento dos dados\n",
    "data_path = '../../data/household_consumption.xlsx'\n",
    "df = pd.read_excel(data_path, sheet_name='Sheet1')\n",
    "\n",
    "print(\"Informações básicas do dataset:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nTipos de dados:\\n{df.dtypes}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação da série temporal\n",
    "df['Date and Hour'] = pd.to_datetime(df['Date and Hour'])\n",
    "df = df.set_index('Date and Hour')\n",
    "df = df.sort_index()\n",
    "\n",
    "# Remover valores nulos se houver\n",
    "print(f\"Valores nulos: {df['Consumption'].isnull().sum()}\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Estatísticas descritivas\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "print(df['Consumption'].describe())\n",
    "\n",
    "# Verificar frequência dos dados\n",
    "freq = pd.infer_freq(df.index)\n",
    "print(f\"\\nFrequência inferida: {freq}\")\n",
    "print(f\"Período total: {df.index.min()} a {df.index.max()}\")\n",
    "print(f\"Duração: {df.index.max() - df.index.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd62716",
   "metadata": {},
   "source": [
    "## 2. Análise Exploratória e Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da série temporal\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Série original\n",
    "axes[0,0].plot(df.index, df['Consumption'], alpha=0.7)\n",
    "axes[0,0].set_title('Série Temporal Original - Consumo de Energia')\n",
    "axes[0,0].set_ylabel('Consumo (kW)')\n",
    "\n",
    "# Histograma\n",
    "axes[0,1].hist(df['Consumption'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Distribuição do Consumo')\n",
    "axes[0,1].set_xlabel('Consumo (kW)')\n",
    "axes[0,1].set_ylabel('Frequência')\n",
    "\n",
    "# Box plot\n",
    "axes[1,0].boxplot(df['Consumption'])\n",
    "axes[1,0].set_title('Box Plot do Consumo')\n",
    "axes[1,0].set_ylabel('Consumo (kW)')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(df['Consumption'], dist=\"norm\", plot=axes[1,1])\n",
    "axes[1,1].set_title('Q-Q Plot (Normalidade)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/01_exploratory_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Teste de normalidade\n",
    "from scipy.stats import jarque_bera, shapiro\n",
    "jb_stat, jb_pvalue = jarque_bera(df['Consumption'])\n",
    "print(f\"\\nTeste Jarque-Bera: estatística={jb_stat:.4f}, p-value={jb_pvalue:.4f}\")\n",
    "print(f\"Série é normal? {jb_pvalue > 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fe917",
   "metadata": {},
   "source": [
    "## 3. Análise de Estacionariedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f15326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries, title):\n",
    "    \"\"\"\n",
    "    Testa estacionariedade usando ADF e KPSS\n",
    "    \"\"\"\n",
    "    print(f'\\n=== {title} ===')\n",
    "    \n",
    "    # Teste ADF\n",
    "    adf_result = adfuller(timeseries, autolag='AIC')\n",
    "    print(f'\\nTeste Augmented Dickey-Fuller:')\n",
    "    print(f'Estatística ADF: {adf_result[0]:.6f}')\n",
    "    print(f'p-value: {adf_result[1]:.6f}')\n",
    "    print(f'Lags usados: {adf_result[2]}')\n",
    "    print(f'Valores críticos:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"=> Série é estacionária (rejeita H0)\")\n",
    "    else:\n",
    "        print(\"=> Série é não-estacionária (não rejeita H0)\")\n",
    "    \n",
    "    # Teste KPSS\n",
    "    kpss_result = kpss(timeseries, regression='c', nlags='auto')\n",
    "    print(f'\\nTeste KPSS:')\n",
    "    print(f'Estatística KPSS: {kpss_result[0]:.6f}')\n",
    "    print(f'p-value: {kpss_result[1]:.6f}')\n",
    "    print(f'Lags usados: {kpss_result[2]}')\n",
    "    print(f'Valores críticos:')\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "    \n",
    "    if kpss_result[1] >= 0.05:\n",
    "        print(\"=> Série é estacionária (não rejeita H0)\")\n",
    "    else:\n",
    "        print(\"=> Série é não-estacionária (rejeita H0)\")\n",
    "\n",
    "# Teste na série original\n",
    "test_stationarity(df['Consumption'], 'Série Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95991e48",
   "metadata": {},
   "source": [
    "## 4. Decomposição da Série Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9517cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reamostragem para análise sazonal (dados por hora para facilitar análise)\n",
    "df_hourly = df['Consumption'].resample('h').mean()\n",
    "\n",
    "# Decomposição sazonal\n",
    "try:\n",
    "    decomposition = seasonal_decompose(df_hourly[:24*7], model='additive', period=24)  # período diário\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    decomposition.observed.plot(ax=axes[0], title='Série Original')\n",
    "    decomposition.trend.plot(ax=axes[1], title='Tendência')\n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Sazonalidade')\n",
    "    decomposition.resid.plot(ax=axes[3], title='Resíduos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/02_seasonal_decomposition.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Análise dos componentes\n",
    "    print(\"Análise dos componentes:\")\n",
    "    print(f\"Variância da série original: {decomposition.observed.var():.4f}\")\n",
    "    print(f\"Variância da tendência: {decomposition.trend.var():.4f}\")\n",
    "    print(f\"Variância sazonal: {decomposition.seasonal.var():.4f}\")\n",
    "    print(f\"Variância dos resíduos: {decomposition.resid.var():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na decomposição: {e}\")\n",
    "    print(\"Continuando sem decomposição detalhada...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c16e7",
   "metadata": {},
   "source": [
    "## 5. Análise de Autocorrelação (ACF e PACF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c36271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar uma amostra menor para ACF/PACF se a série for muito longa\n",
    "sample_size = min(1000, len(df))\n",
    "ts_sample = df['Consumption'].iloc[:sample_size]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ACF e PACF da série original\n",
    "plot_acf(ts_sample, ax=axes[0,0], lags=40, title='ACF - Série Original')\n",
    "plot_pacf(ts_sample, ax=axes[0,1], lags=40, title='PACF - Série Original')\n",
    "\n",
    "# ACF e PACF da primeira diferença\n",
    "ts_diff = ts_sample.diff().dropna()\n",
    "plot_acf(ts_diff, ax=axes[1,0], lags=40, title='ACF - Primeira Diferença')\n",
    "plot_pacf(ts_diff, ax=axes[1,1], lags=40, title='PACF - Primeira Diferença')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/03_acf_pacf_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Teste de estacionariedade na primeira diferença\n",
    "test_stationarity(ts_diff, 'Primeira Diferença')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb7dae",
   "metadata": {},
   "source": [
    "## 6. Transformação Box-Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d19696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transformação Box-Cox (apenas para valores positivos)\n",
    "if (df['Consumption'] > 0).all():\n",
    "    ts_boxcox, lambda_boxcox = boxcox(df['Consumption'])\n",
    "    print(f\"Parâmetro λ da transformação Box-Cox: {lambda_boxcox:.4f}\")\n",
    "    \n",
    "    # Visualizar transformação\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(df.index[:1000], df['Consumption'][:1000])\n",
    "    axes[0].set_title('Série Original')\n",
    "    axes[0].set_ylabel('Consumo')\n",
    "    \n",
    "    axes[1].plot(df.index[:1000], ts_boxcox[:1000])\n",
    "    axes[1].set_title(f'Série Transformada (Box-Cox λ={lambda_boxcox:.3f})')\n",
    "    axes[1].set_ylabel('Consumo Transformado')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/04_boxcox_transformation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Criar série transformada\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed['Consumption'] = ts_boxcox\n",
    "    \n",
    "    # Teste de estacionariedade na série transformada\n",
    "    test_stationarity(ts_boxcox, 'Série Box-Cox Transformada')\n",
    "    \n",
    "else:\n",
    "    print(\"Série contém valores não-positivos. Box-Cox não aplicável.\")\n",
    "    df_transformed = df.copy()\n",
    "    lambda_boxcox = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfab910",
   "metadata": {},
   "source": [
    "## 7. Divisão dos Dados (Treino/Teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc09be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão temporal: 80% treino, 20% teste\n",
    "train_size = int(len(df_transformed) * 0.8)\n",
    "train_data = df_transformed[:train_size]\n",
    "test_data = df_transformed[train_size:]\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_data)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_data)}\")\n",
    "print(f\"Período de treino: {train_data.index.min()} a {train_data.index.max()}\")\n",
    "print(f\"Período de teste: {test_data.index.min()} a {test_data.index.max()}\")\n",
    "\n",
    "# Visualização da divisão\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(train_data.index, train_data['Consumption'], label='Treino', alpha=0.7)\n",
    "plt.plot(test_data.index, test_data['Consumption'], label='Teste', alpha=0.7)\n",
    "plt.title('Divisão Treino/Teste')\n",
    "plt.ylabel('Consumo (transformado)' if lambda_boxcox else 'Consumo')\n",
    "plt.legend()\n",
    "plt.savefig(f'{output_dir}/05_train_test_split.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab3e68",
   "metadata": {},
   "source": [
    "## 8. Busca de Hiperparâmetros SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sarimax_model(train_data, test_data, order, seasonal_order, lambda_param=None):\n",
    "    \"\"\"\n",
    "    Avalia um modelo SARIMAX com parâmetros específicos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ajustar modelo\n",
    "        model = SARIMAX(train_data['Consumption'], \n",
    "                       order=order, \n",
    "                       seasonal_order=seasonal_order,\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "        \n",
    "        fitted_model = model.fit(disp=False, maxiter=100)\n",
    "        \n",
    "        # Previsões\n",
    "        forecast = fitted_model.forecast(steps=len(test_data))\n",
    "        \n",
    "        # Reverter transformação Box-Cox se aplicada\n",
    "        if lambda_param is not None:\n",
    "            forecast_original = inv_boxcox(forecast, lambda_param)\n",
    "            test_original = inv_boxcox(test_data['Consumption'], lambda_param)\n",
    "        else:\n",
    "            forecast_original = forecast\n",
    "            test_original = test_data['Consumption']\n",
    "        \n",
    "        # Métricas\n",
    "        mse = mean_squared_error(test_original, forecast_original)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(test_original, forecast_original)\n",
    "        mape = np.mean(np.abs((test_original - forecast_original) / test_original)) * 100\n",
    "        \n",
    "        return {\n",
    "            'model': fitted_model,\n",
    "            'forecast': forecast_original,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'aic': fitted_model.aic,\n",
    "            'bic': fitted_model.bic\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Grid search para SARIMAX\n",
    "print(\"Iniciando busca de hiperparâmetros...\")\n",
    "\n",
    "# Parâmetros para testar (reduzidos para eficiência)\n",
    "p_values = range(0, 3)\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 3)\n",
    "\n",
    "# Parâmetros sazonais (assumindo sazonalidade diária se houver dados suficientes)\n",
    "seasonal_periods = [24] if len(df_hourly) >= 24*7 else [0]  # 24 horas\n",
    "\n",
    "best_model = None\n",
    "best_aic = float('inf')\n",
    "results = []\n",
    "\n",
    "for p in p_values:\n",
    "    for d in d_values:\n",
    "        for q in q_values:\n",
    "            for sp in seasonal_periods:\n",
    "                if sp > 0:\n",
    "                    seasonal_orders = [(0,1,1,sp), (1,1,1,sp), (1,1,0,sp)]  # Alguns padrões comuns\n",
    "                else:\n",
    "                    seasonal_orders = [(0,0,0,0)]  # Sem sazonalidade\n",
    "                \n",
    "                for seasonal_order in seasonal_orders:\n",
    "                    order = (p, d, q)\n",
    "                    \n",
    "                    result = evaluate_sarimax_model(train_data, test_data, order, seasonal_order, lambda_boxcox)\n",
    "                    \n",
    "                    if result is not None:\n",
    "                        results.append({\n",
    "                            'order': order,\n",
    "                            'seasonal_order': seasonal_order,\n",
    "                            **result\n",
    "                        })\n",
    "                        \n",
    "                        if result['aic'] < best_aic:\n",
    "                            best_aic = result['aic']\n",
    "                            best_model = result\n",
    "                            best_order = order\n",
    "                            best_seasonal = seasonal_order\n",
    "                        \n",
    "                        print(f\"SARIMAX{order}x{seasonal_order} - AIC: {result['aic']:.2f}, RMSE: {result['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nMelhor modelo: SARIMAX{best_order}x{best_seasonal}\")\n",
    "print(f\"AIC: {best_model['aic']:.2f}\")\n",
    "print(f\"RMSE: {best_model['rmse']:.4f}\")\n",
    "print(f\"MAPE: {best_model['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a3aab",
   "metadata": {},
   "source": [
    "## 9. Modelo Final e Diagnósticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525edc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo final com melhores parâmetros\n",
    "final_model = SARIMAX(train_data['Consumption'], \n",
    "                     order=best_order, \n",
    "                     seasonal_order=best_seasonal,\n",
    "                     enforce_stationarity=False,\n",
    "                     enforce_invertibility=False)\n",
    "\n",
    "fitted_final = final_model.fit(disp=False)\n",
    "\n",
    "print(\"\\n=== Resumo do Modelo Final ===\")\n",
    "print(fitted_final.summary())\n",
    "\n",
    "# Diagnósticos dos resíduos\n",
    "residuals = fitted_final.resid\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Resíduos ao longo do tempo\n",
    "axes[0,0].plot(residuals)\n",
    "axes[0,0].set_title('Resíduos ao Longo do Tempo')\n",
    "axes[0,0].set_ylabel('Resíduos')\n",
    "\n",
    "# Histograma dos resíduos\n",
    "axes[0,1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Distribuição dos Resíduos')\n",
    "axes[0,1].set_xlabel('Resíduos')\n",
    "\n",
    "# Q-Q plot dos resíduos\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1,0])\n",
    "axes[1,0].set_title('Q-Q Plot dos Resíduos')\n",
    "\n",
    "# ACF dos resíduos\n",
    "plot_acf(residuals, ax=axes[1,1], lags=30, title='ACF dos Resíduos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/06_residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Testes nos resíduos\n",
    "print(\"\\n=== Testes de Diagnóstico ===\")\n",
    "\n",
    "# Teste de Ljung-Box (independência dos resíduos)\n",
    "lb_result = acorr_ljungbox(residuals, lags=10, return_df=True)\n",
    "print(f\"\\nTeste Ljung-Box:\")\n",
    "print(lb_result)\n",
    "\n",
    "# Teste de normalidade dos resíduos\n",
    "jb_stat, jb_pvalue = jarque_bera(residuals)\n",
    "print(f\"\\nTeste Jarque-Bera (resíduos): estatística={jb_stat:.4f}, p-value={jb_pvalue:.4f}\")\n",
    "print(f\"Resíduos são normais? {jb_pvalue > 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75864d76",
   "metadata": {},
   "source": [
    "## 10. Previsões e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c76dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões no conjunto de teste\n",
    "forecast = fitted_final.forecast(steps=len(test_data))\n",
    "forecast_ci = fitted_final.get_forecast(steps=len(test_data)).conf_int()\n",
    "\n",
    "# Reverter transformação Box-Cox se aplicada\n",
    "if lambda_boxcox is not None:\n",
    "    forecast_original = inv_boxcox(forecast, lambda_boxcox)\n",
    "    test_original = inv_boxcox(test_data['Consumption'], lambda_boxcox)\n",
    "    train_original = inv_boxcox(train_data['Consumption'], lambda_boxcox)\n",
    "    \n",
    "    # Intervalos de confiança (aproximação)\n",
    "    forecast_lower = inv_boxcox(forecast_ci.iloc[:, 0], lambda_boxcox)\n",
    "    forecast_upper = inv_boxcox(forecast_ci.iloc[:, 1], lambda_boxcox)\n",
    "else:\n",
    "    forecast_original = forecast\n",
    "    test_original = test_data['Consumption']\n",
    "    train_original = train_data['Consumption']\n",
    "    forecast_lower = forecast_ci.iloc[:, 0]\n",
    "    forecast_upper = forecast_ci.iloc[:, 1]\n",
    "\n",
    "# Métricas de avaliação\n",
    "mse = mean_squared_error(test_original, forecast_original)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_original, forecast_original)\n",
    "mape = np.mean(np.abs((test_original - forecast_original) / test_original)) * 100\n",
    "r2 = r2_score(test_original, forecast_original)\n",
    "\n",
    "print(\"\\n=== Métricas de Avaliação ===\")\n",
    "print(f\"MSE: {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"R²: {r2:.6f}\")\n",
    "\n",
    "# Visualização das previsões\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plotar apenas últimas observações para melhor visualização\n",
    "n_plot = min(500, len(train_original))\n",
    "plt.plot(train_original.index[-n_plot:], train_original.iloc[-n_plot:], \n",
    "         label='Treino', alpha=0.7)\n",
    "plt.plot(test_original.index, test_original, \n",
    "         label='Teste (Real)', color='green', alpha=0.8)\n",
    "plt.plot(test_data.index, forecast_original, \n",
    "         label='Previsões SARIMAX', color='red', alpha=0.8)\n",
    "plt.fill_between(test_data.index, forecast_lower, forecast_upper, \n",
    "                color='red', alpha=0.2, label='Intervalo de Confiança 95%')\n",
    "\n",
    "plt.title(f'Previsões SARIMAX{best_order}x{best_seasonal} - Consumo de Energia')\n",
    "plt.ylabel('Consumo (kW)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f'{output_dir}/07_sarimax_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47435dd6",
   "metadata": {},
   "source": [
    "## 11. Análise de Validação Cruzada Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação cruzada temporal (Time Series Split)\n",
    "def time_series_cv(data, model_order, seasonal_order, n_splits=5, test_size=0.1):\n",
    "    \"\"\"\n",
    "    Validação cruzada para séries temporais\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    data_size = len(data)\n",
    "    test_samples = int(data_size * test_size)\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        # Calcular índices de divisão\n",
    "        split_idx = int(data_size * (0.5 + i * 0.1))  # Começar com 50% dos dados\n",
    "        \n",
    "        if split_idx + test_samples >= data_size:\n",
    "            break\n",
    "            \n",
    "        train_cv = data[:split_idx]\n",
    "        test_cv = data[split_idx:split_idx + test_samples]\n",
    "        \n",
    "        try:\n",
    "            # Ajustar modelo\n",
    "            model = SARIMAX(train_cv['Consumption'], \n",
    "                           order=model_order, \n",
    "                           seasonal_order=seasonal_order,\n",
    "                           enforce_stationarity=False,\n",
    "                           enforce_invertibility=False)\n",
    "            \n",
    "            fitted = model.fit(disp=False, maxiter=100)\n",
    "            forecast = fitted.forecast(steps=len(test_cv))\n",
    "            \n",
    "            # Reverter Box-Cox se necessário\n",
    "            if lambda_boxcox is not None:\n",
    "                forecast_orig = inv_boxcox(forecast, lambda_boxcox)\n",
    "                test_orig = inv_boxcox(test_cv['Consumption'], lambda_boxcox)\n",
    "            else:\n",
    "                forecast_orig = forecast\n",
    "                test_orig = test_cv['Consumption']\n",
    "            \n",
    "            # Métricas\n",
    "            mse = mean_squared_error(test_orig, forecast_orig)\n",
    "            mae = mean_absolute_error(test_orig, forecast_orig)\n",
    "            mape = np.mean(np.abs((test_orig - forecast_orig) / test_orig)) * 100\n",
    "            \n",
    "            results.append({\n",
    "                'split': i+1,\n",
    "                'mse': mse,\n",
    "                'rmse': np.sqrt(mse),\n",
    "                'mae': mae,\n",
    "                'mape': mape\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no split {i+1}: {e}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Executar validação cruzada\n",
    "print(\"\\n=== Validação Cruzada Temporal ===\")\n",
    "cv_results = time_series_cv(df_transformed, best_order, best_seasonal, n_splits=3)\n",
    "\n",
    "if cv_results:\n",
    "    cv_df = pd.DataFrame(cv_results)\n",
    "    print(\"\\nResultados da Validação Cruzada:\")\n",
    "    print(cv_df)\n",
    "    \n",
    "    print(\"\\nEstatísticas da Validação Cruzada:\")\n",
    "    print(f\"RMSE médio: {cv_df['rmse'].mean():.6f} ± {cv_df['rmse'].std():.6f}\")\n",
    "    print(f\"MAE médio: {cv_df['mae'].mean():.6f} ± {cv_df['mae'].std():.6f}\")\n",
    "    print(f\"MAPE médio: {cv_df['mape'].mean():.2f}% ± {cv_df['mape'].std():.2f}%\")\n",
    "    \n",
    "    # Visualização\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    cv_df['rmse'].plot(kind='bar', ax=axes[0], title='RMSE por Split')\n",
    "    cv_df['mae'].plot(kind='bar', ax=axes[1], title='MAE por Split')\n",
    "    cv_df['mape'].plot(kind='bar', ax=axes[2], title='MAPE por Split')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/08_cross_validation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Não foi possível executar validação cruzada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f269440",
   "metadata": {},
   "source": [
    "## 12. Salvamento do Modelo e Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados em arquivo\n",
    "results_dict = {\n",
    "    'model_type': 'SARIMAX',\n",
    "    'best_order': best_order,\n",
    "    'best_seasonal_order': best_seasonal,\n",
    "    'lambda_boxcox': lambda_boxcox,\n",
    "    'metrics': {\n",
    "        'mse': float(mse),\n",
    "        'rmse': float(rmse),\n",
    "        'mae': float(mae),\n",
    "        'mape': float(mape),\n",
    "        'r2': float(r2),\n",
    "        'aic': float(fitted_final.aic),\n",
    "        'bic': float(fitted_final.bic)\n",
    "    },\n",
    "    'cross_validation': cv_results if cv_results else None,\n",
    "    'data_info': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(train_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'start_date': str(df.index.min()),\n",
    "        'end_date': str(df.index.max())\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{output_dir}/sarimax_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "# Salvar previsões\n",
    "predictions_df = pd.DataFrame({\n",
    "    'real': test_original,\n",
    "    'predicted': forecast_original,\n",
    "    'lower_ci': forecast_lower,\n",
    "    'upper_ci': forecast_upper\n",
    "}, index=test_data.index)\n",
    "\n",
    "predictions_df.to_csv(f'{output_dir}/sarimax_predictions.csv')\n",
    "\n",
    "print(f\"\\n=== Arquivos Salvos ===\")\n",
    "print(f\"Resultados: {output_dir}/sarimax_results.json\")\n",
    "print(f\"Previsões: {output_dir}/sarimax_predictions.csv\")\n",
    "print(f\"Gráficos: {output_dir}/01-08_*.png\")\n",
    "\n",
    "print(f\"\\n=== Resumo Final - SARIMAX ===\")\n",
    "print(f\"Modelo: SARIMAX{best_order}x{best_seasonal}\")\n",
    "print(f\"Transformação Box-Cox: λ = {lambda_boxcox:.4f}\" if lambda_boxcox else \"Sem transformação\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"\\nDiagnósticos:\")\n",
    "print(f\"- Resíduos normais: {jb_pvalue > 0.05}\")\n",
    "print(f\"- AIC: {fitted_final.aic:.2f}\")\n",
    "print(f\"- BIC: {fitted_final.bic:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
